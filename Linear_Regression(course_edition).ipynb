{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c0f7f1-75e6-4188-9bf5-bc6dd0112172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535c3d34-fe0d-4bbc-9b46-b78dd5c8ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds  = MNIST(root = \"data\", train = True, download = True) # for train\n",
    "test_ds  = MNIST(root = \"data\", train = False, download = True) # for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f473cd32-544c-459b-91a8-120df448d45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ca039a-1f30-40bc-bd27-30a123bb639a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a50214-e11c-45fd-b427-b1013fe2a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "im, gt = train_ds[323]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc630c96-f6ff-4fcf-8072-0274b0f89221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAc0lEQVR4nN2QQQ6AIBADV1+2P6M/W382HlBEEBITT/YCYaDtYvZCgU8YQ+YQMzZ0hXEioM8DY2IaD6ZLaVPtT63HKrNtZFqPKDXhutoE5eJqd3k+TzUsecls68IjDyIgui8WIImseyFzKjXseAsg79APtAP1s161LHF/CQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8cdce96-7ea4-436b-a506-c6e8f8ec8c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d0d21c-3c76-4da3-8b85-2a189c882a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image values range from 0~255 \n",
    "# Images can be grayscale and RGB (color). Grayscale images have one channel; and RGB images have 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18c758a1-71bc-4785-ac1e-259de42ac56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 191 255  64\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 191 255 255   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  64 255 255 191   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  64 255 255 191  64   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 255 255 255  64   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 255 255 255 128   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  64 255 255 191   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  64 255 255 191  64   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 191 255 255 128   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 191 255 255 191   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 255 255 255   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 128 255 255 191   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 255 255 255  64   0   0   0   0   0   0   0\n",
      "  128 128 255 128   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 128 255 255 128   0   0   0   0   0   0   0 191\n",
      "  255 255 255 255  64   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 128 255 255 128   0   0   0   0   0  64 255 255\n",
      "  255 255 255 255 191   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  64 255 255 128   0   0   0   0   0 191 255 255\n",
      "  191 191 255 255 191   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 191 255 191   0   0   0   0 128 255 255 255\n",
      "  191 255 255 255  64   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 128 255 255 255 128 128 128 255 255 255 255\n",
      "  255 255 255 128   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  64 255 255 255 255 255 255 255 255 255 255\n",
      "  255 255 128   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 128 255 255 255 255 255 255 255 128\n",
      "   64   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "im_array = np.array(im)\n",
    "print(im_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "521561d9-e394-4419-8fb9-8cf7e891b395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a55d9c2-9cae-458b-88f1-a1486d1cd82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(im, cmap = \"gray\")\n",
    "plt.title(f\"GT -> {gt}\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d60f28d-d360-46e5-b2a9-018331eed1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation (transformation) library\n",
    "from torchvision import transforms as T\n",
    "\n",
    "def get_ds(root):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This function get a to save data and returns train and test datasets.\n",
    "    \n",
    "    Parameter:\n",
    "        \n",
    "        root    - path to save the data, str;\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize transformations (augmentations)\n",
    "    tr_tfs = T.Compose( [T.RandomHorizontalFlip(),  # horizontal flip\n",
    "                         T.ToTensor(), # PIL Image -> tensor; (0~1) # (im_h, im_w, im_ch) -> (im_ch, im_h, im_w)\n",
    "                         T.Normalize(mean = 0.5, std = 0.5) ] ) # 1 channel bo'lgani uchun bitta mean/std value ishlatiladi \n",
    "    \n",
    "    test_tfs = T.Compose( [T.ToTensor(), # PIL Image -> tensor; (0~1) # (im_w, im_h, ch) -> (im_ch, im_h, im_w)\n",
    "                           T.Normalize(mean = 0.5, std = 0.5) ] ) # (0~1) -> (0~255)\n",
    "    \n",
    "    train_ds  = MNIST(root = root, train = True, transform = tr_tfs, download = True) # for train \n",
    "    test_ds  = MNIST(root = root, train = False, transform = test_tfs, download = True) # for test\n",
    "    \n",
    "    return train_ds, test_ds\n",
    "\n",
    "ds, test_ds = get_ds(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01fa0b41-8e26-40b5-beee-c549b67844b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000, -0.9922, -0.9843, -0.9765, -0.9294, -0.9137, -0.8902, -0.8745,\n",
       "        -0.8588, -0.8196, -0.8118, -0.8039, -0.7961, -0.7882, -0.7647, -0.7255,\n",
       "        -0.7176, -0.6941, -0.6627, -0.6471, -0.6392, -0.6157, -0.5686, -0.5608,\n",
       "        -0.4980, -0.4824, -0.4510, -0.3882, -0.3725, -0.3647, -0.3569, -0.2941,\n",
       "        -0.2706, -0.2627, -0.1608, -0.1529, -0.1059, -0.0667, -0.0118, -0.0039,\n",
       "         0.0196,  0.0353,  0.0431,  0.0588,  0.0667,  0.0902,  0.1608,  0.1765,\n",
       "         0.2078,  0.2235,  0.2549,  0.3020,  0.3333,  0.3412,  0.3490,  0.3725,\n",
       "         0.4275,  0.4353,  0.4588,  0.4667,  0.4902,  0.5294,  0.5529,  0.5765,\n",
       "         0.6078,  0.6235,  0.6627,  0.6706,  0.7176,  0.7333,  0.7647,  0.7725,\n",
       "         0.7961,  0.8667,  0.8824,  0.8902,  0.8980,  0.9137,  0.9373,  0.9529,\n",
       "         0.9608,  0.9686,  0.9765,  0.9843,  1.0000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im, gt = ds[0]\n",
    "torch.unique(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6392ca09-8a54-4e11-ab63-924fd063c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88aeebd4-742c-4611-9527-868012c128c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000\n",
      "6000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "tr_length = int(len(ds) * 0.9) # 90% of ds\n",
    "tr_ds, val_ds = random_split(ds, [tr_length, len(ds) - tr_length] )\n",
    "print(len(tr_ds)); print(len(val_ds)); print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47120501-f33e-4705-97a6-349c17640020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu da data bitta bitta ko'rib chiqiladi; gpu da esa parallel ko'rib chiqiladi\n",
    "# misol uchun cpu 1 sekundda 1ta image ko'rsa; gpu 1 sekundda (batch) ta image ni ko'ra oladi -> parallel computation ; size of mini_batch depends on gpu memory\n",
    "# batch lar bu data ni qismlar ga bo'linggan holati; 54000 ta image bo'lsa mini_batch_size = 100, 54000 / 100 = 540 ta batch lardan iborat bo'ladi;\n",
    "# batch larni yaratib beradigan function bu dataloader; Pytorrch da train qilayotganimizda dataset asosida dataloader ni yaratishimiz kerak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10799dd9-0be1-4cb1-be0b-3263da373799",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128; # size of mini batch odatda 2 ni darajasi sifatida olinadi; 256, 512, 1024 ...\n",
    "tr_dl = DataLoader(dataset = tr_ds, batch_size = bs, shuffle = True, num_workers = 8, drop_last = False) # data ni avval cpu bilan chaqirib olamiz; num_workers = cpu ni toshlari soni\n",
    "val_dl = DataLoader(dataset = val_ds, batch_size = bs, shuffle = False, num_workers = 8)\n",
    "test_dl = DataLoader(dataset = test_ds, batch_size = bs, shuffle = False, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5133be26-eab1-47ea-a945-d65a2e9da56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n",
      "47\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "# Assignment \n",
    "print(len(tr_dl)); print(len(val_dl)); print(len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87144d6d-4961-4a53-967b-1025b8363b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(tr_dl))\n",
    "print(batch[0].shape); print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4de39-6309-458b-b34c-2a9255fb6c22",
   "metadata": {},
   "source": [
    "# Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aad94f07-2f2c-47ce-a0d7-44b5223563a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 3)\n",
    "print(x.shape)\n",
    "print(x.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "666af309-eaee-4ed6-9f17-bc1b2210904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 3) # (1, 3) x (3, 2) = (1, 2) # (1, 7) -> 1-sample da 7ta feature bor degani : education, lunch, math_score ... \n",
    "w = torch.rand(2, 3)\n",
    "print(torch.matmul(x, w.T).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e36d1d9-f4ad-428b-9df3-778450285607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(in_features = 3, out_features = 2) # (20 + 30 + 40) -> (44.843 + 45.8454) -> (90)\n",
    "print(lin(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61711112-6373-4ebf-bafa-3d8e0cf57f9b",
   "metadata": {},
   "source": [
    "# torch input shape -> (bs, im_ch, im_h, im_w) -> (4D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf125e8d-2dbb-4589-9f3c-b4b07eb3ddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0923, 0.1002, 0.0938, 0.1052, 0.0968, 0.1107, 0.1073, 0.0999, 0.0909,\n",
      "         0.1030]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class CustomModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_feats, out_feats, num_classes): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_layer_1 = torch.nn.Linear(in_features = in_feats, out_features = out_feats)\n",
    "        self.activation = torch.nn.ReLU() \n",
    "        self.linear_layer_2 = torch.nn.Linear(in_features = out_feats, out_features = out_feats // 2)\n",
    "        self.out_layer = torch.nn.Linear(in_features = out_feats // 2, out_features = num_classes)\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, inp): \n",
    "        \n",
    "        # inp_shape = 4D (bs, im_ch, im_h, im_w) 4D -> 2D\n",
    "        bs = inp.shape[0]\n",
    "        inp = inp.view(bs, -1) # (bs, im_ch, im_h, im_w) -> (bs, im_ch * im_h * im_w) 4D -> 2D\n",
    "        inp = self.linear_layer_1(inp) # input: (bs, 784); output: (bs, 392) \n",
    "        inp = self.activation(inp) # doesnot change feature dimensions input: (bs, 392); output: (bs, 392) \n",
    "        inp = self.linear_layer_2(inp) # input: (bs, 392); output: (bs, 392 // 2 = 196)  \n",
    "        out = self.out_layer(inp)\n",
    "        \n",
    "        return self.softmax(out)\n",
    "    \n",
    "inp = torch.rand(1, 1, 28, 28) # (bs, im_ch, im_h, im_w) -> 4D -> 2D (bs, features)\n",
    "model = CustomModel(in_feats = 28 * 28 * 1, out_feats = (28 * 28) // 2, num_classes = 10) # 784 -> 1568 - X ; 784 -> 392\n",
    "print(model(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e5ec63-e21e-4089-ae5f-7fd2cac17758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_layer_1.weight parameterda 307328 ta parameter bor.\n",
      "linear_layer_1.bias parameterda 392 ta parameter bor.\n",
      "linear_layer_2.weight parameterda 76832 ta parameter bor.\n",
      "linear_layer_2.bias parameterda 196 ta parameter bor.\n",
      "out_layer.weight parameterda 1960 ta parameter bor.\n",
      "out_layer.bias parameterda 10 ta parameter bor.\n",
      "Modelning umumiy parameter lar soni -> 386718\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(f\"{name} parameterda {params.numel()} ta parameter bor.\")\n",
    "print(f\"Modelning umumiy parameter lar soni -> {sum(params.numel() for params in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d3a638c-ab6d-483e-af89-cf5c16f58a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "vals = torch.arange(5, 10, 1)\n",
    "print(vals)\n",
    "relu_outs = torch.nn.ReLU()(vals)\n",
    "print(relu_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5eb557d-a679-4422-be1a-4ecd5c2931fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0754, -0.1027, -0.0945,  0.0784,  0.0233, -0.1027,  0.1343, -0.0463,\n",
      "         0.0369,  0.1287])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40578/2741397163.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.Softmax()(outputs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0925, 0.0901, 0.0908, 0.1079, 0.1021, 0.0901, 0.1141, 0.0953, 0.1035,\n",
       "        0.1135])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = torch.FloatTensor([-0.0754, -0.1027, -0.0945,  0.0784,  0.0233, -0.1027,  0.1343, -0.0463, 0.0369,  0.1287])\n",
    "print(outputs)\n",
    "torch.nn.Softmax()(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103096b6-6fd6-4f2a-a09f-c7e091208507",
   "metadata": {},
   "source": [
    "### Epoch is when a model sees all examples once. \n",
    "### Images in the dataset = 100; Batch_size = 10; Iteration - 100 // 10 = 10? In this case, 10 iterations == 1 epoch;\n",
    "### Images in the dataset = 100; Batch_size = 10; Epoch = 10; Iteration - 1 epoch has (100 // 10) = 10 iterations, then 10 epochs have 10 * 10 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "724415f2-f6f7-4024-8285-6c8194f7e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1 = w0 - lr * w0.grad # w0 = 3, w1 = 70\n",
    "\n",
    "epochs = 10\n",
    "device = \"cuda\" # \"cuda\"\n",
    "model.to(device) # model is automatically on cpu at first. so we need to switch to gpu\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b0155c4-bfb3-464d-b1e6-3db0fb721561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1 = w0 - lr * w0.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af7c703a-c223-407e-8138-6d215a6145b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train boshlandi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 239.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train is finished!\n",
      "Epoch 1 train loss -> 1.698523831028509!\n",
      "Epoch 1 train acc -> 0.7702037037037037!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation is finished!\n",
      "Epoch 1 validation loss -> 1.6199148360719071!\n",
      "Epoch 1 validation acc -> 0.8446666666666667!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 313.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train is finished!\n",
      "Epoch 2 train loss -> 1.588779675452065!\n",
      "Epoch 2 train acc -> 0.8755925925925926!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation is finished!\n",
      "Epoch 2 validation loss -> 1.5636549173517431!\n",
      "Epoch 2 validation acc -> 0.8998333333333334!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 306.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train is finished!\n",
      "Epoch 3 train loss -> 1.5595376652563917!\n",
      "Epoch 3 train acc -> 0.9034074074074074!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 validation is finished!\n",
      "Epoch 3 validation loss -> 1.551784670099299!\n",
      "Epoch 3 validation acc -> 0.9106666666666666!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 299.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train is finished!\n",
      "Epoch 4 train loss -> 1.5495021394078765!\n",
      "Epoch 4 train acc -> 0.9127407407407407!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 validation is finished!\n",
      "Epoch 4 validation loss -> 1.5458354696314385!\n",
      "Epoch 4 validation acc -> 0.916!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 305.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train is finished!\n",
      "Epoch 5 train loss -> 1.5434262961572944!\n",
      "Epoch 5 train acc -> 0.9183518518518519!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 validation is finished!\n",
      "Epoch 5 validation loss -> 1.5363565150727616!\n",
      "Epoch 5 validation acc -> 0.9256666666666666!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 303.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train is finished!\n",
      "Epoch 6 train loss -> 1.5386328375170015!\n",
      "Epoch 6 train acc -> 0.9227777777777778!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 validation is finished!\n",
      "Epoch 6 validation loss -> 1.5373565694119067!\n",
      "Epoch 6 validation acc -> 0.9241666666666667!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 306.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train is finished!\n",
      "Epoch 7 train loss -> 1.5338049692000257!\n",
      "Epoch 7 train acc -> 0.9275185185185185!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 validation is finished!\n",
      "Epoch 7 validation loss -> 1.5519828263749467!\n",
      "Epoch 7 validation acc -> 0.9085!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 293.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train is finished!\n",
      "Epoch 8 train loss -> 1.5360728716963274!\n",
      "Epoch 8 train acc -> 0.9252777777777778!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 validation is finished!\n",
      "Epoch 8 validation loss -> 1.539476085216441!\n",
      "Epoch 8 validation acc -> 0.9206666666666666!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 309.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train is finished!\n",
      "Epoch 9 train loss -> 1.5306534134381191!\n",
      "Epoch 9 train acc -> 0.9305!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 validation is finished!\n",
      "Epoch 9 validation loss -> 1.5280655089845048!\n",
      "Epoch 9 validation acc -> 0.9328333333333333!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [00:01, 312.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train is finished!\n",
      "Epoch 10 train loss -> 1.5279931459946654!\n",
      "Epoch 10 train acc -> 0.9329814814814815!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 validation is finished!\n",
      "Epoch 10 validation loss -> 1.5436700328867485!\n",
      "Epoch 10 validation acc -> 0.9175!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"Train boshlandi\")\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # if epoch == 1: break\n",
    "    \n",
    "    epoch_loss, epoch_acc, total = 0, 0, 0\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(tr_dl)):\n",
    "        \n",
    "        ims, gts = batch # ims and gts are automatically on cpu at first. so we need to switch to gpu\n",
    "        ims, gts = ims.to(device), gts.to(device)\n",
    "        \n",
    "        preds = model(ims)\n",
    "        # logits\n",
    "        loss = loss_fn(preds, gts) # loss is tensor -> float value\n",
    "        # print(loss)\n",
    "        \n",
    "        total += ims.shape[0] # bs (bs, im_chs, im_h, im_w)\n",
    "        pred_val, pred_cls = torch.max(preds.data, dim = 1)\n",
    "        # if idx == 0: break\n",
    "        epoch_acc += (pred_cls == gts).sum().item() # bs ta pred dan nechtasi gts ga tengligini hisoblaydi: 128tadan  100tasi gt ga teng bo'lsa = 100 + 110 + 120 + 137\n",
    "        epoch_loss += loss.item() # 2 + 2 + 2 = 2 + 2 + 2 + 2 10ta son chiqadi / 10\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # batch_size ta image uchun hisoblangan\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} train is finished!\")\n",
    "    print(f\"Epoch {epoch + 1} train loss -> {epoch_loss / len(tr_dl)}!\")\n",
    "    print(f\"Epoch {epoch + 1} train acc -> {epoch_acc / total}!\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_epoch_loss, val_epoch_acc, val_total = 0, 0, 0\n",
    "        \n",
    "        for idx, batch in enumerate(val_dl):\n",
    "            ims, gts = batch\n",
    "            ims, gts = ims.to(device), gts.to(device)\n",
    "            val_total += ims.shape[0]\n",
    "            \n",
    "            preds = model(ims) # oputputs are probabilities\n",
    "            loss = loss_fn(preds, gts)\n",
    "            _, pred_cls = torch.max(preds.data, dim = 1)\n",
    "            val_epoch_loss += loss.item()\n",
    "            val_epoch_acc += (pred_cls == gts).sum().item()\n",
    "            \n",
    "        val_acc = val_epoch_acc / val_total\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} validation is finished!\")\n",
    "        print(f\"Epoch {epoch + 1} validation loss -> {val_epoch_loss / len(val_dl)}!\")\n",
    "        print(f\"Epoch {epoch + 1} validation acc -> {val_acc}!\") \n",
    "        \n",
    "        # 1-epochda val_acc = 60; best_acc = 0: 60 > 0; best_acc = 60\n",
    "        # 2-epochda val_acc = 65; best_acc = 60: 65 > 60; best_acc = 65\n",
    "        # 3-epochda val_acc = 63; best_acc = 65: 63 > 65 will not run\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            os.makedirs(\"saved_models\", exist_ok = True)\n",
    "            torch.save(model.state_dict(), f\"saved_models/mnist_best_model.pth\") # model.state_dict() returns trained weights and biases\n",
    "            best_acc = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b0bb5-1212-4a72-a1d3-44306500bfe2",
   "metadata": {},
   "source": [
    "### Inference (Test Phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12025dc8-c21f-4226-b393-e2651195ab6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's trainable parameters are succesfully loaded!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (linear_layer_1): Linear(in_features=784, out_features=392, bias=True)\n",
       "  (activation): ReLU()\n",
       "  (linear_layer_2): Linear(in_features=392, out_features=196, bias=True)\n",
       "  (out_layer): Linear(in_features=196, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"saved_models/mnist_best_model.pth\"))\n",
    "print(\"Model's trainable parameters are succesfully loaded!\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af116358-d35a-4fc2-860b-730031e152f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "ims, gts = next(iter(test_dl))\n",
    "# print(ims.shape)\n",
    "# print(gts)\n",
    "\n",
    "preds, images, lbls = [], [], []\n",
    "\n",
    "for im, gt in zip(ims, gts):\n",
    "    im, gt = im.to(device), gt.to(device)\n",
    "    preds_ = model(im) # 10 ta prob scores\n",
    "    _, pred_cls = torch.max(preds_, dim = 1)\n",
    "    images.append(im)\n",
    "    preds.append(pred_cls.item())\n",
    "    lbls.append(gt)\n",
    "print(len(preds))\n",
    "print(len(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ae2bfb4-ab9d-4c25-a635-507ea74ad793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute transpose for high dimensions: 1,28,28 -> 28,28,1\n",
    "def tensor_2_im(t): return (t * 255).detach().cpu().permute(1,2,0).numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "881962d0-0533-48f0-9596-5dff487e2ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = 2\n",
    "num_ims = 10\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "indekslar = np.random.randint(low = 0, high = len(preds), size = num_ims)\n",
    "\n",
    "for idx, indeks in enumerate(indekslar):\n",
    "    # if idx == 1: break\n",
    "    im = ims[int(indeks)]\n",
    "    gt = gts[int(indeks)]\n",
    "    pred = preds[int(indeks)]\n",
    "    \n",
    "    plt.subplot(row, num_ims // row, idx + 1)\n",
    "    plt.imshow(tensor_2_im(im), cmap = 'gray')\n",
    "    # plt.imshow(im, cmap = 'gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"GT -> {gt}; Pred -> {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba219954-98a6-49cd-b28c-a7ffb7a25f2a",
   "metadata": {},
   "source": [
    "### Class#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d16a69e-3ea6-4fb9-b0db-0108f2b7a4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "out1 = torch.tensor(0)\n",
    "out2 = torch.tensor(0.00002)\n",
    "outs = torch.FloatTensor([out1, out2])\n",
    "sig = Sigmoid()\n",
    "# sig = torch.nn.Softmax()\n",
    "sig(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e946807-1cce-462e-9946-9fcd63e81f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0954, 0.0900, 0.1045, 0.0987, 0.1098, 0.0941, 0.1076, 0.1132, 0.0935,\n",
      "         0.0933]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class CustomModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_feats, out_feats, num_classes): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_layer_1 = torch.nn.Linear(in_features = in_feats, out_features = out_feats)\n",
    "        self.activation = torch.nn.ReLU() \n",
    "        self.linear_layer_2 = torch.nn.Linear(in_features = out_feats, out_features = out_feats // 2)\n",
    "        self.out_layer = torch.nn.Linear(in_features = out_feats // 2, out_features = num_classes)\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, inp): \n",
    "        \n",
    "        # inp_shape = 4D (bs, im_ch, im_h, im_w) 4D -> 2D\n",
    "        # Pseudo-preprocessing\n",
    "        bs = inp.shape[0]\n",
    "        inp = inp.view(bs, -1) # (bs, im_ch, im_h, im_w) -> (bs, im_ch * im_h * im_w) 4D -> 2D\n",
    "        \n",
    "        # Model starts from here\n",
    "        inp = self.linear_layer_1(inp) # input: (bs, 784); output: (bs, 392) \n",
    "        inp = self.activation(inp) # doesnot change feature dimensions input: (bs, 392); output: (bs, 392) \n",
    "        inp = self.linear_layer_2(inp) # input: (bs, 392); output: (bs, 392 // 2 = 196)  \n",
    "        out = self.out_layer(inp)\n",
    "        \n",
    "        return self.softmax(out)\n",
    "    \n",
    "inp = torch.rand(1, 1, 28, 28) # (bs, im_ch, im_h, im_w) -> 4D -> 2D (bs, features)\n",
    "model = CustomModel(in_feats = 28 * 28 * 1, out_feats = (28 * 28) // 2, num_classes = 10) # 784 -> 1568 - X ; 784 -> 392\n",
    "print(model(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18570dae-2f25-4958-b1b0-454c0f165d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "# Linear layerga input doim 2D bo'lishi kerak, shulardan birinchi doim bs bo'ladi.  \n",
    "# (bs, im_ch, im_h, im_w) - tensor input dimensions\n",
    "inp_1 = torch.rand(10, 100) \n",
    "\n",
    "lin = Linear(in_features = 100, \n",
    "             out_features = 1000)\n",
    "\n",
    "lin(inp_1).shape\n",
    "\n",
    "# img (im_h = 200, im_w = 200)  -> (im_h = 100, im_w = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d569f1-a107-4f1f-9c3d-730e7aecb813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagen",
   "language": "python",
   "name": "imagen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
